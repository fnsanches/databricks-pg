{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e5204f-3a02-4a7e-accd-6c77dcfa6828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d481e3a0-261f-4f9f-bbc0-579327e9d672/lib/python3.11/site-packages (37.4.0)\nRequirement already satisfied: tzdata in /databricks/python3/lib/python3.11/site-packages (from faker) (2022.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be4f85cd-8a47-461e-9db0-fb4c9d931d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import DataSource, DataSourceReader\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.datasource import InputPartition\n",
    "\n",
    "class RangePartition(InputPartition):\n",
    "    def __init__(self, start, end):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "class FakeDataSourceReader(DataSourceReader):\n",
    "\n",
    "    def __init__(self, schema, options):\n",
    "        self.schema: StructType = schema\n",
    "        self.options = options\n",
    "\n",
    "    def partitions(self):\n",
    "        return [RangePartition(1, 10000),\n",
    "                RangePartition(10001, 20000),\n",
    "                RangePartition(20001, 30000),\n",
    "                RangePartition(30001, 40000),\n",
    "                 RangePartition(40001, 60000),\n",
    "                ]\n",
    "\n",
    "    def read(self, partition):\n",
    "        # Library imports must be within the method.\n",
    "        from faker import Faker\n",
    "        import random\n",
    "        import datetime\n",
    "        fake = Faker()\n",
    "\n",
    "        # Every value in this `self.options` dictionary is a string.\n",
    "        # for client in clients:\n",
    "        card_operations = []    \n",
    "        num_transactions = random.randint(partition.start, partition.end)\n",
    "        for _ in range(num_transactions):\n",
    "            transaction_id = fake.unique.uuid4()\n",
    "            start_date = datetime.datetime.strptime(\"2000-01-01\", '%Y-%m-%d')\n",
    "            client_id = random.randint(1, 2000)\n",
    "            tran_date = fake.date_between(\n",
    "                start_date=start_date,\n",
    "                end_date=datetime.timedelta(days=1)\n",
    "                )\n",
    "            operation = ( \n",
    "                str(transaction_id),\n",
    "                str(client_id),\n",
    "                str(fake.random_int(min=1, max=100000)/10),\n",
    "                tran_date.strftime('%Y-%m-%d'),\n",
    "                fake.company(),\n",
    "                random.choice(\n",
    "                    [\"approved\", \"approved\",\"approved\",\"approved\",\"approved\",\"approved\",\"approved\",\"approved\",\"approved\",\"approved\",\"declined\"]\n",
    "                    )\n",
    "            )\n",
    "            yield  operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98cde36-d4b5-4bb5-845a-aa2ce56bb536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import DataSource, DataSourceReader\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "class FakeDataSource(DataSource):\n",
    "    \"\"\"\n",
    "    An example data source for batch query using the `faker` library.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return \"fake\"\n",
    "\n",
    "    def schema(self):\n",
    "        return \"transaction_id string, card_id string, transaction_amount string, transaction_date string, merchant string, status string\"\n",
    "\n",
    "    def reader(self, schema: StructType):\n",
    "        return FakeDataSourceReader(schema, self.options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a23c5a-7fae-4563-87f5-2db216ad373c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.dataSource.register(FakeDataSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b374e78e-bcf0-44ae-a4ac-29201f7818a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "df = spark.read.format(\"fake\").load()\n",
    "ts = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "df.write.mode(\"append\").parquet(f\"/Volumes/sandbox/bronze/landing/{ts}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Fake Transactions Data Ingestion with PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}